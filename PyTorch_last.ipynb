{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664520439464,
     "user": {
      "displayName": "Владимир Днепровский",
      "userId": "12512107289711404401"
     },
     "user_tz": -180
    },
    "id": "pzgyJhXAtLzC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Recall\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3452,
     "status": "ok",
     "timestamp": 1664520444384,
     "user": {
      "displayName": "Владимир Днепровский",
      "userId": "12512107289711404401"
     },
     "user_tz": -180
    },
    "id": "o9N4JbcWudk2"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C:/Users/vovad/Desktop/All_projects/Конкурс_Новосиб/dataset_novosib/train.csv\", nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как распределение данных в классах сильно неравномерно, то добавляем синтетические данные для малых классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df_train[['Easting','Northing','Height','Reflectance']], df_train['Class']\n",
    "sampling_strategy = {4: 300000, 1: 300000, 5: 300000, 64: 300000}\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy)\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "\n",
    "df_train = pd.concat((x_smote, y_smote), ignore_index=False, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логарифмируем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Easting_log'], df_train['Northing_log'], df_train['Height_log'], df_train['Reflectance_log'] = np.log10((df_train['Easting'], df_train['Northing'], df_train['Height'], (df_train['Reflectance']+45) ))\n",
    "df_train = df_train.drop(['Easting', 'Northing', 'Height', 'Reflectance'], axis=1)\n",
    "df_train = df_train[[col for col in df_train.columns if col != 'Class'] + ['Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сбалансируем классы, чтобы нейросеть оценивала их равномерно. (Класс 64, для удобства one-hot encoding, назовем классом 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train[\"Class\"] == 64, \"Class\"] = 2\n",
    "\n",
    "num_0 = len(df_train.loc[(df_train[\"Class\"] == 0)])\n",
    "num_1 = len(df_train.loc[(df_train[\"Class\"] == 1)])\n",
    "num_2 = len(df_train.loc[(df_train[\"Class\"] == 2)])\n",
    "num_3 = len(df_train.loc[(df_train[\"Class\"] == 3)])\n",
    "num_4 = len(df_train.loc[(df_train[\"Class\"] == 4)])\n",
    "num_5 = len(df_train.loc[(df_train[\"Class\"] == 5)])\n",
    "\n",
    "total_nums = num_0 + num_1 + num_2 + num_3 + num_4 + num_5\n",
    "\n",
    "weight_0 = 1/(num_0/total_nums)/2\n",
    "weight_1 = 1/(num_1/total_nums)/2\n",
    "weight_2 = 1/(num_2/total_nums)/2\n",
    "weight_3 = 1/(num_3/total_nums)/2\n",
    "weight_4 = 1/(num_4/total_nums)/2\n",
    "weight_5 = 1/(num_5/total_nums)/2\n",
    "\n",
    "total_weight = np.array([weight_0, weight_1, weight_2, weight_3, weight_4, weight_5])\n",
    "\n",
    "total_weight = torch.from_numpy(total_weight).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем датасет на тренировочный и валидационный "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.iloc[:, 0:-1]\n",
    "y = df_train.iloc[:, -1]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синтезируем дополнительные признаки с помощью функции PolynomialFeatures() и нормализуем данные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "polier = PolynomialFeatures(3)\n",
    "X_train = polier.fit_transform(X_train)\n",
    "X_val = polier.transform(X_val)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем экземпляры функций синтезатора и нормализатора (не знаю как их по-русски назвать :)), так как они понадобятся для тестового набора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/notebooks/Scalers/scaler_120.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(polier, '/notebooks/Scalers/polier.gz')\n",
    "joblib.dump(scaler, '/notebooks/Scalers/scaler.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем класс преобразования датасетов в тензоры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем переменные для гиперпараметров нейросети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = 35\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем модель нашей сети. Возможно модель покажется избыточной, но я не пробовал уменьшать емкость. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LidarModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, sign_size=32, cha_input=16, cha_hidden=32, \n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = sign_size*cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size//2\n",
    "        output_size = (sign_size//4) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = conv1 = nn.Conv1d(\n",
    "            cha_input, \n",
    "            cha_input*K, \n",
    "            kernel_size=5, \n",
    "            stride = 1, \n",
    "            padding=2,  \n",
    "            groups=cha_input, \n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input*K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input*K, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_hidden)\n",
    "        conv3 = nn.Conv1d(\n",
    "            cha_hidden, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv3 = nn.utils.weight_norm(conv3, dim=None)        \n",
    "\n",
    "        # 4th conv layer\n",
    "        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)\n",
    "        conv4 = nn.Conv1d(\n",
    "            cha_hidden, \n",
    "            cha_hidden, \n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2, \n",
    "            groups=cha_hidden, \n",
    "            bias=False)\n",
    "        self.conv4 = nn.utils.weight_norm(conv4, dim=None)\n",
    "\n",
    "        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm2 = nn.BatchNorm1d(output_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_output)\n",
    "        dense2 = nn.Linear(output_size, output_dim, bias=True)\n",
    "        self.dense2 = nn.utils.weight_norm(dense2)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = nn.functional.celu(self.dense1(x))\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "\n",
    "        x = self.batch_norm_c4(x)\n",
    "        x = self.conv4(x)\n",
    "        x =  x + x_s\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.avg_po_c4(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)       \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка cuda или cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем экземпляр модели, функцию потерь и оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LidarModel(\n",
      "  (batch_norm1): BatchNorm1d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (dense1): Linear(in_features=35, out_features=1024, bias=False)\n",
      "  (batch_norm_c1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
      "  (ave_po_c1): AdaptiveAvgPool1d(output_size=8)\n",
      "  (batch_norm_c2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_c2): Dropout(p=0.3, inplace=False)\n",
      "  (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "  (batch_norm_c3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_c3): Dropout(p=0.3, inplace=False)\n",
      "  (conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "  (batch_norm_c4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
      "  (avg_po_c4): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  (flt): Flatten(start_dim=1, end_dim=-1)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.4, inplace=False)\n",
      "  (dense2): Linear(in_features=256, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LidarModel(\n",
    "    input_dim=NUM_FEATURES, \n",
    "    output_dim=NUM_CLASSES, \n",
    "    sign_size=16, \n",
    "    cha_input=64, \n",
    "    cha_hidden=64, \n",
    "    K=2, \n",
    "    dropout_input=0.3, \n",
    "    dropout_hidden=0.3,\n",
    "    dropout_output=0.4\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=total_weight).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем функцию метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_recall(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "      \n",
    "    recall = Recall(average='macro', num_classes=6).to(device)    \n",
    "    rec = recall(y_pred_tags, y_test)\n",
    "    \n",
    "    rec = torch.round(rec * 100)\n",
    "    \n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем путь сохранения модели, на случай сбоев или потери обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'C:/Users/vovad/Desktop/All_projects/Конкурс_Новосиб/model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin training.\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS+1):    \n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_rec = 0\n",
    "        for X_train_batch, y_train_batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            train_loss = criterion(y_train_pred, y_train_batch)\n",
    "            # train_rec = multi_recall(y_train_pred, y_train_batch)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            train_epoch_loss += train_loss.item()\n",
    "            # train_epoch_rec += train_rec.item()            \n",
    "            \n",
    "        with torch.inference_mode():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_rec = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                # val_rec = multi_recall(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                # val_epoch_rec += val_rec.item()\n",
    "                \n",
    "        if val_loss < val_epoch_loss:\n",
    "            print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "            torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)\n",
    "            \n",
    "        loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "        loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "        recall_stats['train'].append(train_epoch_rec/len(train_loader))\n",
    "        recall_stats['val'].append(val_epoch_rec/len(val_loader))\n",
    "    \n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch {epoch+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Recall:{train_epoch_rec/len(train_loader):.3f}| Val Recall:{val_epoch_rec/len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOAD_PATH = '/notebooks/Model/model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы загружаем модель для получения предсказаний на новом датасете, \n",
    "необходимо установить гиперпараметры и создать модель нейросети путем повторного запуска соответствующих ячеек выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае сбоя или других причин создаем новый экземпляр модели и загружаем в него сохраненную модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LidarModel(\n",
       "  (batch_norm1): BatchNorm1d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       "  (dense1): Linear(in_features=35, out_features=1024, bias=False)\n",
       "  (batch_norm_c1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
       "  (ave_po_c1): AdaptiveAvgPool1d(output_size=8)\n",
       "  (batch_norm_c2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_c2): Dropout(p=0.3, inplace=False)\n",
       "  (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "  (batch_norm_c3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_c3): Dropout(p=0.3, inplace=False)\n",
       "  (conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "  (batch_norm_c4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
       "  (avg_po_c4): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "  (flt): Flatten(start_dim=1, end_dim=-1)\n",
       "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.4, inplace=False)\n",
       "  (dense2): Linear(in_features=256, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = LidarModel(\n",
    "    input_dim=NUM_FEATURES, \n",
    "    output_dim=NUM_CLASSES, \n",
    "    sign_size=16, \n",
    "    cha_input=64, \n",
    "    cha_hidden=64, \n",
    "    K=2, \n",
    "    dropout_input=0.3, \n",
    "    dropout_hidden=0.3,\n",
    "    dropout_output=0.4\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(f=MODEL_LOAD_PATH))\n",
    "model_loaded.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем оценку модели на тестовых данных. Загрузим тестовый датасет и преобразуем его как мы это делали с тренировочным датасетом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_raw = pd.read_csv(\"/notebooks/Dataset/test_dataset_test.csv\")\n",
    "df_test_raw['Easting_log'], df_test_raw['Northing_log'], df_test_raw['Height_log'], df_test_raw['Reflectance_log'] = np.log10((df_test_raw['Easting'], df_test_raw['Northing'], \n",
    "                                                                                                                   df_test_raw['Height'], (df_test_raw['Reflectance']+45) ))\n",
    "\n",
    "df_test_raw = df_test_raw.drop(['id', 'Easting', 'Northing', 'Height', 'Reflectance'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим синтезатор и нормализатор, если это необходимо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "polier = joblib.load('/notebooks/Scalers/polier.gz')\n",
    "scaler = joblib.load('/notebooks/Scalers/scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = polier.transform(df_test_raw)\n",
    "df_test = scaler.transform(df_test)\n",
    "\n",
    "class ClassifierDatasetTest(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data       \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "test_dataset = ClassifierDatasetTest(torch.from_numpy(df_test).float())\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем предсказания с помощью нашей натренированной модели, если мы загружаем ранее сохраненную модель, то надо поменять model на model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)        \n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)        \n",
    "        y_pred_list.append(y_pred_tags.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем предсказания в необходимый формат для загрузки на сайт. (Не забываем вернуть класс 2 обратно в класс 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "y_pred_list1 = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_pred_list2 = list(itertools.chain.from_iterable(y_pred_list1))\n",
    "cols = ['id']\n",
    "df_test1 = pd.read_csv(\"/notebooks/Dataset/test_dataset_test.csv\", usecols=cols)\n",
    "df_test1['Class'] = [a for a in y_pred_list2]\n",
    "df_test1.loc[df_test1[\"Class\"] == 2, \"Class\"] = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем файл в формате csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.to_csv('/notebooks/Dataset/test_check_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6d089313d9964bf0a775057fdba2ddb76b757f887813491471703a7ee59e2bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
